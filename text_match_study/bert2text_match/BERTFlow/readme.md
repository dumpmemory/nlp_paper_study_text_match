# 【关于 BERT-flow 】那些你不知道的事 

> 作者：杨夕
> 
> 项目地址：https://github.com/km1994/nlp_paper_study
> 
> 个人介绍：大佬们好，我叫杨夕，该项目主要是本人在研读顶会论文和复现经典论文过程中，所见、所思、所想、所闻，可能存在一些理解错误，希望大佬们多多指正。
> 
> 论文：On the Sentence Embeddings from Pre-trained Language Models
> 
> 会议：EMNLP2020
> 
> 论文地址：https://arxiv.org/pdf/2011.05864.pdf
> 
> 论文代码：https://github.com/bohanli/BERT-flow

## 目录

- [【关于 BERT-flow 】那些你不知道的事](#关于-bert-flow-那些你不知道的事)
  - [目录](#目录)
  - [摘要](#摘要)
  - [参考](#参考)

## 摘要

- 前沿：像BERT这样的经过预训练的上下文表示在自然语言处理中取得了巨大的成功；
- 动机：已经发现，未经微调的来自预训练语言模型的句子嵌入很难捕获句子的语义；
- 论文方法：在本文中，我们认为BERT嵌入中的语义信息没有得到充分利用。我们首先从理论上揭示了掩盖的语言模型预训练目标与语义相似性任务之间的理论联系，然后从经验上分析了BERT句子的嵌入。
- 实验结果：我们发现BERT总是诱发非光滑的各向异性语义空间，这会损害其语义相似性的表现。为解决此问题，我们建议通过将非正则化的流量标准化来将各向异性的语义嵌入分布转换为平滑的各向异性高斯分布。实验结果表明，我们提出的BERT流方法在各种语义文本相似性任务上比最先进的句子嵌入方法具有明显的性能提升。



## 参考

1. [文本匹配、文本相似度模型之ESIM](https://blog.csdn.net/u012526436/article/details/90380840)
2. [短文本匹配的利器-ESIM](https://zhuanlan.zhihu.com/p/47580077)